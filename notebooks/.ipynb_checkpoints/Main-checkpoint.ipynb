{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as pySql\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_invoices_path = '../notebooks/data/product_invoices/'\n",
    "product_package_types_path = '../notebooks/data/product_package_types/'\n",
    "product_shipments_path = '../notebooks/data/product_shipments/'\n",
    "provider_invoices_path = '../notebooks/data/provider_invoices/'\n",
    "provider_prices_path = '../notebooks/data/provider_prices/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def read_json(path):\n",
    "    return spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_invoices = read_json(product_invoices_path)\n",
    "product_package_types = read_json(product_package_types_path)\n",
    "product_shipments = read_json(product_shipments_path)\n",
    "provider_invoices = read_json(provider_invoices_path)\n",
    "provider_prices = read_json(provider_prices_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- transaction_id: long (nullable = true)\n",
      " |-- user_invoice_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_invoices.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- buyer_id: long (nullable = true)\n",
      " |-- from_country: string (nullable = true)\n",
      " |-- package_type_id: long (nullable = true)\n",
      " |-- seller_id: long (nullable = true)\n",
      " |-- shipping_label_created: string (nullable = true)\n",
      " |-- to_country: string (nullable = true)\n",
      " |-- tracking_code: string (nullable = true)\n",
      " |-- transaction_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_shipments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_invoices_full = product_invoices.join(product_shipments, ['transaction_id'])\\\n",
    "                                        .select(product_invoices.amount,\n",
    "                                                product_shipments.tracking_code)\\\n",
    "                                        .withColumnRenamed(\"amount\",\"vinted_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_invoices_compare = provider_invoices.select(provider_invoices.amount, \n",
    "                                                     provider_invoices.tracking_code)\\\n",
    "                                             .withColumnRenamed('amount','provider_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = product_invoices_full.join(provider_invoices_compare, ['tracking_code'], 'left')\n",
    "\n",
    "discrepencies_in_amount = comparison_df.filter(pySql.col('vinted_amount')!=pySql.col('provider_amount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrepencies_in_amount_adv_prv = comparison_df.filter(pySql.col('vinted_amount')<pySql.col('provider_amount'))\n",
    "discrepencies_in_amount_adv_vnt = comparison_df.filter(pySql.col('vinted_amount')>pySql.col('provider_amount'))\n",
    "total_amount_discrepency = comparison_df.select(pySql.sum(pySql.col('vinted_amount'))-pySql.sum(pySql.col('provider_amount')))\\\n",
    "                                        .collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_invoices:  554038\n",
      "provider_invoices:  562943\n",
      "discrepencies_in_amount:  187602\n",
      "provider charges more than Vinted:  68266\n",
      "provider charges less than Vinted:  119336\n",
      "total amount discrepency:  5717.439252016367\n"
     ]
    }
   ],
   "source": [
    "print('product_invoices: ', product_invoices.count())\n",
    "print('provider_invoices: ', provider_invoices.count())\n",
    "print('discrepencies_in_amount: ', discrepencies_in_amount.count())\n",
    "print('provider charges more than Vinted: ', discrepencies_in_amount_adv_prv.count())\n",
    "print('provider charges less than Vinted: ', discrepencies_in_amount_adv_vnt.count())\n",
    "print('total amount discrepency: ', total_amount_discrepency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I.\n",
    "● Are there any discrepancies between the shipping service provider invoices and our invoices to buyers? Can you explain them?\n",
    "● What are the key trends you observe in the data over the period for which data is available? What is the business impact of these trends?\n",
    "● If you had more time and could ask for more data, how would you continue your analysis to identify and fix shortcomings in the shipping/billing process?\n",
    "\n",
    "● After exploring data I see that discrepency is amount paid for delivery, around 33% of product invoice amount are mismatched with provider amount. Most of them are in advantage for Vinted. This in my opiniond derives from different delivery amount calculations on vinted and provider side. product_package_types dataset is not that detailed as provider_prices is. This creates discrepencies on mentioned KPI.\n",
    "● With additional data and time I would try to create more precise package delivery pricing classification, that could be more accurate to country, region, provider etc. so billing would be as near as it possibly can be to what service provider charges Vinted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../notebooks/arch_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Please think about the production version of your solution and provide a structure of how the ETL pipeline could look like to arrive from the raw input data to the analytical reporting. Think of potential risks regarding the scalability and flexibility of your solution having the rapid growth of Vinted in mind.\n",
    "\n",
    "Pipeline really depends on how data is provided by sources. Pipeline could be triggered by file system where data is dropped (if possible) or by schedule, not overloading ETL job with any business logic or quality checks just to increate performance on getting data into place. Best scenario would be getting delta of data and adding it to existing raw data. If possible, avoid full loads. From bronze to silver layer jobs should clear data, remove or separate corrupted data and do most of the data quality checks (if for that jupyter notebooks are used I would suggest writing out custom libs adjusted to particular working patterns). Last job should be developed together with business person, no data discrepancies should arrive to it, only changes  should be just simple joins to complete data usability, setting number precision, column names etc.\n",
    "\n",
    "Performance issues: Increasing amount of data could be solved by parallelizing jobs, scaling clusters and adjusting time windows, Spark does provide capability to scale according to your requirements.\n",
    "In Data Lake I would suggest to Parquet or Delta Lake formats, those provide quite scalable solutions for most day to day perf issues. Regular data offload jobs should also improve performance of some datasets, moving unused data to “cold” or historical storages can save time and cost.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
